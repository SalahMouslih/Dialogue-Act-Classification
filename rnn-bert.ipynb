{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom transformers import AutoTokenizer, AutoModel","metadata":{"execution":{"iopub.status.busy":"2023-03-11T20:24:33.359150Z","iopub.execute_input":"2023-03-11T20:24:33.359723Z","iopub.status.idle":"2023-03-11T20:24:44.428721Z","shell.execute_reply.started":"2023-03-11T20:24:33.359649Z","shell.execute_reply":"2023-03-11T20:24:44.427475Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModel, BertTokenizer, BertModel\nimport torch\nfrom torch.utils.data import DataLoader, Dataset\nfrom datasets import load_dataset","metadata":{"execution":{"iopub.status.busy":"2023-03-11T20:24:44.430675Z","iopub.execute_input":"2023-03-11T20:24:44.431432Z","iopub.status.idle":"2023-03-11T20:24:45.373604Z","shell.execute_reply.started":"2023-03-11T20:24:44.431380Z","shell.execute_reply":"2023-03-11T20:24:45.372384Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# Load the SWDA dataset\nswda = load_dataset(\"swda\")\n\nswda","metadata":{"execution":{"iopub.status.busy":"2023-03-11T20:24:45.375129Z","iopub.execute_input":"2023-03-11T20:24:45.376182Z","iopub.status.idle":"2023-03-11T20:26:11.186371Z","shell.execute_reply.started":"2023-03-11T20:24:45.376143Z","shell.execute_reply":"2023-03-11T20:26:11.185295Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/7.11k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fc561161b5604a0b9c75cd033c2ca75e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading metadata:   0%|          | 0.00/2.54k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d60bcd578d70406cba54def7807ef791"}},"metadata":{}},{"name":"stdout","text":"Downloading and preparing dataset swda/default (download: 13.79 MiB, generated: 158.13 MiB, post-processed: Unknown size, total: 171.91 MiB) to /root/.cache/huggingface/datasets/swda/default/0.0.0/b53d17ec4c6e31d0921591dd2d8e86d15850822209a980fcddb2983fc948e499...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/14.4M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9462c88ed4cd4cf885fcb61002d4dfb3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data files:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3f1d93fa88c1463295dcd8ebda1b89f6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/2.24k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"124015d1bc30459f814e36edb4fde30b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/579 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"92e8c393b8464470a6200c56e6e7fd2a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/75.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5fa49d412ccd40379fef12a85f43b299"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting data files:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"65701a3bc123414d996cb7b29a088a6b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/213543 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/56729 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/4514 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Dataset swda downloaded and prepared to /root/.cache/huggingface/datasets/swda/default/0.0.0/b53d17ec4c6e31d0921591dd2d8e86d15850822209a980fcddb2983fc948e499. Subsequent calls will reuse this data.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4f962f4adeaa45da8bf722daa3cbed91"}},"metadata":{}},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['swda_filename', 'ptb_basename', 'conversation_no', 'transcript_index', 'act_tag', 'damsl_act_tag', 'caller', 'utterance_index', 'subutterance_index', 'text', 'pos', 'trees', 'ptb_treenumbers', 'talk_day', 'length', 'topic_description', 'prompt', 'from_caller', 'from_caller_sex', 'from_caller_education', 'from_caller_birth_year', 'from_caller_dialect_area', 'to_caller', 'to_caller_sex', 'to_caller_education', 'to_caller_birth_year', 'to_caller_dialect_area'],\n        num_rows: 213543\n    })\n    validation: Dataset({\n        features: ['swda_filename', 'ptb_basename', 'conversation_no', 'transcript_index', 'act_tag', 'damsl_act_tag', 'caller', 'utterance_index', 'subutterance_index', 'text', 'pos', 'trees', 'ptb_treenumbers', 'talk_day', 'length', 'topic_description', 'prompt', 'from_caller', 'from_caller_sex', 'from_caller_education', 'from_caller_birth_year', 'from_caller_dialect_area', 'to_caller', 'to_caller_sex', 'to_caller_education', 'to_caller_birth_year', 'to_caller_dialect_area'],\n        num_rows: 56729\n    })\n    test: Dataset({\n        features: ['swda_filename', 'ptb_basename', 'conversation_no', 'transcript_index', 'act_tag', 'damsl_act_tag', 'caller', 'utterance_index', 'subutterance_index', 'text', 'pos', 'trees', 'ptb_treenumbers', 'talk_day', 'length', 'topic_description', 'prompt', 'from_caller', 'from_caller_sex', 'from_caller_education', 'from_caller_birth_year', 'from_caller_dialect_area', 'to_caller', 'to_caller_sex', 'to_caller_education', 'to_caller_birth_year', 'to_caller_dialect_area'],\n        num_rows: 4514\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"# Define the data preprocessing function\ndef preprocess_data(utterance):\n    text = utterance['text']\n    label = utterance['damsl_act_tag']\n    return {'text': text, 'label': label}\n# Preprocess the data\nswda = swda.map(preprocess_data)","metadata":{"execution":{"iopub.status.busy":"2023-03-11T20:26:11.189405Z","iopub.execute_input":"2023-03-11T20:26:11.190467Z","iopub.status.idle":"2023-03-11T20:27:18.151170Z","shell.execute_reply.started":"2023-03-11T20:26:11.190417Z","shell.execute_reply":"2023-03-11T20:27:18.150037Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/213543 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1fafb0cc2f7946cd8db8bdc842ea052d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/56729 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"53001e34fb1f47ab9474ca4afafd0486"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/4514 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9e5697403221417a8d9dec3ba45c3e05"}},"metadata":{}}]},{"cell_type":"code","source":"swda = swda.remove_columns(['swda_filename', 'ptb_basename', 'conversation_no', 'transcript_index', 'act_tag', 'damsl_act_tag', 'caller', 'utterance_index', 'subutterance_index', 'pos', 'trees', 'ptb_treenumbers', 'talk_day', 'length', 'topic_description', 'prompt', 'from_caller', 'from_caller_sex', 'from_caller_education', 'from_caller_birth_year', 'from_caller_dialect_area', 'to_caller', 'to_caller_sex', 'to_caller_education', 'to_caller_birth_year', 'to_caller_dialect_area'])","metadata":{"execution":{"iopub.status.busy":"2023-03-11T20:27:18.155996Z","iopub.execute_input":"2023-03-11T20:27:18.158569Z","iopub.status.idle":"2023-03-11T20:27:18.189064Z","shell.execute_reply.started":"2023-03-11T20:27:18.158528Z","shell.execute_reply":"2023-03-11T20:27:18.188125Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"swda","metadata":{"execution":{"iopub.status.busy":"2023-03-11T20:27:18.193428Z","iopub.execute_input":"2023-03-11T20:27:18.196309Z","iopub.status.idle":"2023-03-11T20:27:18.206757Z","shell.execute_reply.started":"2023-03-11T20:27:18.196267Z","shell.execute_reply":"2023-03-11T20:27:18.205751Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['text', 'label'],\n        num_rows: 213543\n    })\n    validation: Dataset({\n        features: ['text', 'label'],\n        num_rows: 56729\n    })\n    test: Dataset({\n        features: ['text', 'label'],\n        num_rows: 4514\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"import pandas as pd\nsorted(pd.DataFrame(swda[\"train\"])[\"label\"].unique())","metadata":{"execution":{"iopub.status.busy":"2023-03-11T20:27:18.211193Z","iopub.execute_input":"2023-03-11T20:27:18.214045Z","iopub.status.idle":"2023-03-11T20:27:29.959806Z","shell.execute_reply.started":"2023-03-11T20:27:18.214006Z","shell.execute_reply":"2023-03-11T20:27:29.958723Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"[0,\n 1,\n 2,\n 3,\n 4,\n 5,\n 6,\n 7,\n 8,\n 9,\n 10,\n 11,\n 12,\n 13,\n 14,\n 15,\n 16,\n 17,\n 18,\n 19,\n 20,\n 21,\n 22,\n 23,\n 24,\n 25,\n 26,\n 27,\n 28,\n 29,\n 30,\n 31,\n 32,\n 33,\n 34,\n 35,\n 36,\n 37,\n 38,\n 39,\n 40,\n 41,\n 42]"},"metadata":{}}]},{"cell_type":"code","source":"# Tokenize the input and context\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nswda = swda.map(lambda x: tokenizer(x['text'], padding='max_length', truncation=True), batched=True)","metadata":{"execution":{"iopub.status.busy":"2023-03-11T20:27:29.961363Z","iopub.execute_input":"2023-03-11T20:27:29.961825Z","iopub.status.idle":"2023-03-11T20:29:55.472240Z","shell.execute_reply.started":"2023-03-11T20:27:29.961754Z","shell.execute_reply":"2023-03-11T20:29:55.471183Z"},"trusted":true},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"82e15bbbe37d499294105d27715700ac"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3914c89c35d84909aba645afa83f921f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9eaa798084bb45c29c9fe0f201ce2910"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/214 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fc7ba526d6964e60a3ad2b75ea4323f2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/57 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"16a8974b7d9e466db578fa4582076b4d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/5 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"31c1426a29c04993a735e41955048287"}},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModel\nimport torch\nfrom torch.utils.data import DataLoader, Dataset\nfrom datasets import load_dataset\n\n\nclass SWDADataset(Dataset):\n    def __init__(self, swda, tokenizer, max_seq_length):\n        self.swda = swda\n        self.tokenizer = tokenizer\n        self.max_seq_length = max_seq_length\n\n    def __len__(self):\n        return len(self.swda)\n\n    def __getitem__(self, index):\n        # Get input text\n        input_text = self.tokenizer.encode_plus(\n            self.swda[index]['text'],\n            add_special_tokens=True,\n            max_length=self.max_seq_length,\n            padding='max_length',\n            truncation=True,\n            return_tensors='pt'\n        )\n\n        # Get context\n        context = self.swda[index-1] if index > 0 else self.swda[index]\n        context = self.tokenizer.encode_plus(\n            context['text'],\n            add_special_tokens=True,\n            max_length=self.max_seq_length,\n            padding='max_length',\n            truncation=True,\n            return_tensors='pt'\n        )\n\n        # Get label\n        label = self.swda[index]['label']\n        \n        return input_text, context, label\n\n\n# Create the train loader\ntrain_dataset = SWDADataset(swda['train'], tokenizer, max_seq_length=128)\ntrain_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2023-03-11T20:53:42.729309Z","iopub.execute_input":"2023-03-11T20:53:42.729685Z","iopub.status.idle":"2023-03-11T20:53:42.740565Z","shell.execute_reply.started":"2023-03-11T20:53:42.729652Z","shell.execute_reply":"2023-03-11T20:53:42.739227Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"train_dataset[0]","metadata":{"execution":{"iopub.status.busy":"2023-03-11T20:29:55.492155Z","iopub.execute_input":"2023-03-11T20:29:55.494181Z","iopub.status.idle":"2023-03-11T20:29:55.555399Z","shell.execute_reply.started":"2023-03-11T20:29:55.494139Z","shell.execute_reply":"2023-03-11T20:29:55.554367Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"({'input_ids': tensor([[ 101, 3100, 1012, 1013,  102,    0,    0,    0,    0,    0,    0,    0,\n             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n             0,    0,    0,    0,    0,    0,    0,    0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0]])},\n {'input_ids': tensor([[ 101, 3100, 1012, 1013,  102,    0,    0,    0,    0,    0,    0,    0,\n             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n             0,    0,    0,    0,    0,    0,    0,    0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0]])},\n 26)"},"metadata":{}}]},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"execution":{"iopub.status.busy":"2023-03-11T20:29:55.556811Z","iopub.execute_input":"2023-03-11T20:29:55.557144Z","iopub.status.idle":"2023-03-11T20:29:55.629872Z","shell.execute_reply.started":"2023-03-11T20:29:55.557107Z","shell.execute_reply":"2023-03-11T20:29:55.628815Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# Define the model architecture\n# Define model architecture\nclass RNN(nn.Module):\n    def __init__(self, hidden_size, num_classes):\n        super(RNN, self).__init__()\n        self.bert = BertModel.from_pretrained('bert-base-uncased')\n        for param in self.bert.parameters():\n            param.requires_grad = False\n        self.rnn = nn.GRU(input_size=768, hidden_size=hidden_size, batch_first=True)\n        self.fc1 = nn.Linear(hidden_size, hidden_size)\n        self.fc2 = nn.Linear(hidden_size * 2, num_classes)\n\n    def forward(self, input_text, context):\n        input_ids = input_text['input_ids'].to(device)\n        attention_mask = input_text['attention_mask'].to(device)\n        context_ids = context['input_ids'].to(device)\n        context_mask = context['attention_mask'].to(device)\n        \n        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask, return_dict=True)\n        pooled_output = outputs.pooler_output\n\n        context_outputs = self.bert(input_ids=context_ids, attention_mask=context_mask, return_dict=True)\n        pooled_context_output = context_outputs.pooler_output\n        \n        \n        output, _ = self.rnn(pooled_output.unsqueeze(1))\n        context_output, _ = self.rnn(pooled_context_output.unsqueeze(1))\n        output = self.fc1(output)\n        output = torch.cat((output.squeeze(1), context_output.squeeze(1)), dim=1)\n        output = self.fc2(output)\n        return output\n\n# Instantiate the model\n# Instantiate model and move it to device\nmodel = RNN(hidden_size=256, num_classes=43)\nmodel = model.to(device)","metadata":{"execution":{"iopub.status.busy":"2023-03-11T20:53:49.014900Z","iopub.execute_input":"2023-03-11T20:53:49.015444Z","iopub.status.idle":"2023-03-11T20:53:50.750140Z","shell.execute_reply.started":"2023-03-11T20:53:49.015373Z","shell.execute_reply":"2023-03-11T20:53:50.749116Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stderr","text":"Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"}]},{"cell_type":"code","source":"# Define loss function and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)","metadata":{"execution":{"iopub.status.busy":"2023-03-11T20:53:53.391038Z","iopub.execute_input":"2023-03-11T20:53:53.391399Z","iopub.status.idle":"2023-03-11T20:53:53.400632Z","shell.execute_reply.started":"2023-03-11T20:53:53.391367Z","shell.execute_reply":"2023-03-11T20:53:53.397911Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"import os\n#os.environ[\"CUDA_LAUNCH_BLOCKING\"]= \"true\"","metadata":{"execution":{"iopub.status.busy":"2023-03-11T20:30:12.299358Z","iopub.execute_input":"2023-03-11T20:30:12.299724Z","iopub.status.idle":"2023-03-11T20:30:12.309476Z","shell.execute_reply.started":"2023-03-11T20:30:12.299686Z","shell.execute_reply":"2023-03-11T20:30:12.308449Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"num_epochs = 2\n\nfor epoch in range(2):\n    epoch_loss = 0\n    epoch_acc = 0\n\n    model.train()\n\n    for i, (input_text, context, labels) in enumerate(train_loader):\n        optimizer.zero_grad()\n\n        input_text = {\n            'input_ids': input_text['input_ids'].squeeze(1),\n            'attention_mask': input_text['attention_mask'].squeeze(1),\n            'token_type_ids': input_text['token_type_ids'].squeeze(1)\n        }\n        context = {\n            'input_ids': context['input_ids'].squeeze(1),\n            'attention_mask': context['attention_mask'].squeeze(1),\n            'token_type_ids': context['token_type_ids'].squeeze(1)\n        }\n        labels = labels.to(device)\n        output = model(input_text, context)\n        loss = criterion(output, labels)\n\n        loss.backward()\n        optimizer.step()\n\n        epoch_loss += loss.item() * input_text[\"input_ids\"].size(0)\n\n        # Compute accuracy for this batch\n        predicted_labels = output.argmax(dim=1)\n        num_correct = (predicted_labels == labels).sum().item()\n        acc = num_correct / len(labels)\n        epoch_acc += acc\n        if (i+1) % 10 == 0:\n            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Train Loss: {loss.item():.4f}')\n\n    # Compute average loss and accuracy for epoch\n    epoch_loss /= len(train_loader)\n    epoch_acc /= len(train_loader)\n\n    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.4f}')\n","metadata":{"execution":{"iopub.status.busy":"2023-03-11T20:54:05.979547Z","iopub.execute_input":"2023-03-11T20:54:05.979949Z","iopub.status.idle":"2023-03-11T22:03:18.233502Z","shell.execute_reply.started":"2023-03-11T20:54:05.979908Z","shell.execute_reply":"2023-03-11T22:03:18.232438Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"Epoch [1/2], Step [10/1669], Train Loss: 2.4357\nEpoch [1/2], Step [20/1669], Train Loss: 2.2255\nEpoch [1/2], Step [30/1669], Train Loss: 2.1171\nEpoch [1/2], Step [40/1669], Train Loss: 1.9117\nEpoch [1/2], Step [50/1669], Train Loss: 2.2662\nEpoch [1/2], Step [60/1669], Train Loss: 2.0626\nEpoch [1/2], Step [70/1669], Train Loss: 1.7341\nEpoch [1/2], Step [80/1669], Train Loss: 2.1219\nEpoch [1/2], Step [90/1669], Train Loss: 1.9040\nEpoch [1/2], Step [100/1669], Train Loss: 1.8535\nEpoch [1/2], Step [110/1669], Train Loss: 1.8127\nEpoch [1/2], Step [120/1669], Train Loss: 1.7258\nEpoch [1/2], Step [130/1669], Train Loss: 1.7514\nEpoch [1/2], Step [140/1669], Train Loss: 1.6888\nEpoch [1/2], Step [150/1669], Train Loss: 1.6475\nEpoch [1/2], Step [160/1669], Train Loss: 1.7293\nEpoch [1/2], Step [170/1669], Train Loss: 2.1157\nEpoch [1/2], Step [180/1669], Train Loss: 1.4383\nEpoch [1/2], Step [190/1669], Train Loss: 1.5995\nEpoch [1/2], Step [200/1669], Train Loss: 1.6898\nEpoch [1/2], Step [210/1669], Train Loss: 1.5719\nEpoch [1/2], Step [220/1669], Train Loss: 1.5988\nEpoch [1/2], Step [230/1669], Train Loss: 1.6922\nEpoch [1/2], Step [240/1669], Train Loss: 1.3156\nEpoch [1/2], Step [250/1669], Train Loss: 1.5881\nEpoch [1/2], Step [260/1669], Train Loss: 1.6617\nEpoch [1/2], Step [270/1669], Train Loss: 1.5200\nEpoch [1/2], Step [280/1669], Train Loss: 1.6642\nEpoch [1/2], Step [290/1669], Train Loss: 1.7022\nEpoch [1/2], Step [300/1669], Train Loss: 1.7294\nEpoch [1/2], Step [310/1669], Train Loss: 1.3952\nEpoch [1/2], Step [320/1669], Train Loss: 1.4274\nEpoch [1/2], Step [330/1669], Train Loss: 1.4528\nEpoch [1/2], Step [340/1669], Train Loss: 1.6058\nEpoch [1/2], Step [350/1669], Train Loss: 1.5217\nEpoch [1/2], Step [360/1669], Train Loss: 1.5763\nEpoch [1/2], Step [370/1669], Train Loss: 1.2339\nEpoch [1/2], Step [380/1669], Train Loss: 1.6146\nEpoch [1/2], Step [390/1669], Train Loss: 1.6373\nEpoch [1/2], Step [400/1669], Train Loss: 1.3446\nEpoch [1/2], Step [410/1669], Train Loss: 1.5466\nEpoch [1/2], Step [420/1669], Train Loss: 1.1368\nEpoch [1/2], Step [430/1669], Train Loss: 1.4642\nEpoch [1/2], Step [440/1669], Train Loss: 1.4110\nEpoch [1/2], Step [450/1669], Train Loss: 1.5691\nEpoch [1/2], Step [460/1669], Train Loss: 1.6420\nEpoch [1/2], Step [470/1669], Train Loss: 1.2220\nEpoch [1/2], Step [480/1669], Train Loss: 1.2299\nEpoch [1/2], Step [490/1669], Train Loss: 1.3197\nEpoch [1/2], Step [500/1669], Train Loss: 1.7535\nEpoch [1/2], Step [510/1669], Train Loss: 1.4001\nEpoch [1/2], Step [520/1669], Train Loss: 1.5086\nEpoch [1/2], Step [530/1669], Train Loss: 1.4308\nEpoch [1/2], Step [540/1669], Train Loss: 1.3506\nEpoch [1/2], Step [550/1669], Train Loss: 1.2379\nEpoch [1/2], Step [560/1669], Train Loss: 1.1452\nEpoch [1/2], Step [570/1669], Train Loss: 1.2658\nEpoch [1/2], Step [580/1669], Train Loss: 1.2818\nEpoch [1/2], Step [590/1669], Train Loss: 1.4174\nEpoch [1/2], Step [600/1669], Train Loss: 1.1536\nEpoch [1/2], Step [610/1669], Train Loss: 1.3224\nEpoch [1/2], Step [620/1669], Train Loss: 1.3265\nEpoch [1/2], Step [630/1669], Train Loss: 1.5375\nEpoch [1/2], Step [640/1669], Train Loss: 1.4268\nEpoch [1/2], Step [650/1669], Train Loss: 1.3194\nEpoch [1/2], Step [660/1669], Train Loss: 1.2309\nEpoch [1/2], Step [670/1669], Train Loss: 1.3457\nEpoch [1/2], Step [680/1669], Train Loss: 1.4254\nEpoch [1/2], Step [690/1669], Train Loss: 1.4048\nEpoch [1/2], Step [700/1669], Train Loss: 1.6451\nEpoch [1/2], Step [710/1669], Train Loss: 1.4848\nEpoch [1/2], Step [720/1669], Train Loss: 1.3270\nEpoch [1/2], Step [730/1669], Train Loss: 1.4084\nEpoch [1/2], Step [740/1669], Train Loss: 1.3513\nEpoch [1/2], Step [750/1669], Train Loss: 1.3199\nEpoch [1/2], Step [760/1669], Train Loss: 1.2447\nEpoch [1/2], Step [770/1669], Train Loss: 1.2402\nEpoch [1/2], Step [780/1669], Train Loss: 1.3841\nEpoch [1/2], Step [790/1669], Train Loss: 1.2865\nEpoch [1/2], Step [800/1669], Train Loss: 1.4174\nEpoch [1/2], Step [810/1669], Train Loss: 1.0733\nEpoch [1/2], Step [820/1669], Train Loss: 1.3108\nEpoch [1/2], Step [830/1669], Train Loss: 1.0347\nEpoch [1/2], Step [840/1669], Train Loss: 1.1930\nEpoch [1/2], Step [850/1669], Train Loss: 1.3355\nEpoch [1/2], Step [860/1669], Train Loss: 1.3248\nEpoch [1/2], Step [870/1669], Train Loss: 1.1613\nEpoch [1/2], Step [880/1669], Train Loss: 1.1747\nEpoch [1/2], Step [890/1669], Train Loss: 1.1927\nEpoch [1/2], Step [900/1669], Train Loss: 1.3583\nEpoch [1/2], Step [910/1669], Train Loss: 1.1571\nEpoch [1/2], Step [920/1669], Train Loss: 1.1060\nEpoch [1/2], Step [930/1669], Train Loss: 1.3495\nEpoch [1/2], Step [940/1669], Train Loss: 1.3597\nEpoch [1/2], Step [950/1669], Train Loss: 1.4971\nEpoch [1/2], Step [960/1669], Train Loss: 1.4452\nEpoch [1/2], Step [970/1669], Train Loss: 1.4009\nEpoch [1/2], Step [980/1669], Train Loss: 1.3469\nEpoch [1/2], Step [990/1669], Train Loss: 1.4111\nEpoch [1/2], Step [1000/1669], Train Loss: 1.2627\nEpoch [1/2], Step [1010/1669], Train Loss: 1.3306\nEpoch [1/2], Step [1020/1669], Train Loss: 1.2616\nEpoch [1/2], Step [1030/1669], Train Loss: 1.5479\nEpoch [1/2], Step [1040/1669], Train Loss: 1.2700\nEpoch [1/2], Step [1050/1669], Train Loss: 1.2814\nEpoch [1/2], Step [1060/1669], Train Loss: 1.1744\nEpoch [1/2], Step [1070/1669], Train Loss: 1.1874\nEpoch [1/2], Step [1080/1669], Train Loss: 1.1731\nEpoch [1/2], Step [1090/1669], Train Loss: 1.2286\nEpoch [1/2], Step [1100/1669], Train Loss: 1.2887\nEpoch [1/2], Step [1110/1669], Train Loss: 1.1677\nEpoch [1/2], Step [1120/1669], Train Loss: 1.1496\nEpoch [1/2], Step [1130/1669], Train Loss: 1.1217\nEpoch [1/2], Step [1140/1669], Train Loss: 1.1955\nEpoch [1/2], Step [1150/1669], Train Loss: 1.4105\nEpoch [1/2], Step [1160/1669], Train Loss: 1.2780\nEpoch [1/2], Step [1170/1669], Train Loss: 1.1401\nEpoch [1/2], Step [1180/1669], Train Loss: 1.1406\nEpoch [1/2], Step [1190/1669], Train Loss: 1.2273\nEpoch [1/2], Step [1200/1669], Train Loss: 1.4390\nEpoch [1/2], Step [1210/1669], Train Loss: 1.4186\nEpoch [1/2], Step [1220/1669], Train Loss: 1.1666\nEpoch [1/2], Step [1230/1669], Train Loss: 1.2558\nEpoch [1/2], Step [1240/1669], Train Loss: 1.5133\nEpoch [1/2], Step [1250/1669], Train Loss: 1.3372\nEpoch [1/2], Step [1260/1669], Train Loss: 1.1175\nEpoch [1/2], Step [1270/1669], Train Loss: 1.1537\nEpoch [1/2], Step [1280/1669], Train Loss: 1.2559\nEpoch [1/2], Step [1290/1669], Train Loss: 1.2592\nEpoch [1/2], Step [1300/1669], Train Loss: 1.0558\nEpoch [1/2], Step [1310/1669], Train Loss: 1.1605\nEpoch [1/2], Step [1320/1669], Train Loss: 1.2034\nEpoch [1/2], Step [1330/1669], Train Loss: 1.4007\nEpoch [1/2], Step [1340/1669], Train Loss: 1.0512\nEpoch [1/2], Step [1350/1669], Train Loss: 1.2442\nEpoch [1/2], Step [1360/1669], Train Loss: 0.8439\nEpoch [1/2], Step [1370/1669], Train Loss: 1.1305\nEpoch [1/2], Step [1380/1669], Train Loss: 1.2600\nEpoch [1/2], Step [1390/1669], Train Loss: 1.0970\nEpoch [1/2], Step [1400/1669], Train Loss: 1.4561\nEpoch [1/2], Step [1410/1669], Train Loss: 1.4598\nEpoch [1/2], Step [1420/1669], Train Loss: 0.9516\nEpoch [1/2], Step [1430/1669], Train Loss: 1.2933\nEpoch [1/2], Step [1440/1669], Train Loss: 1.5265\nEpoch [1/2], Step [1450/1669], Train Loss: 1.2344\nEpoch [1/2], Step [1460/1669], Train Loss: 1.1711\nEpoch [1/2], Step [1470/1669], Train Loss: 1.1433\nEpoch [1/2], Step [1480/1669], Train Loss: 1.2822\nEpoch [1/2], Step [1490/1669], Train Loss: 1.1906\nEpoch [1/2], Step [1500/1669], Train Loss: 1.1554\nEpoch [1/2], Step [1510/1669], Train Loss: 0.9522\nEpoch [1/2], Step [1520/1669], Train Loss: 1.1631\nEpoch [1/2], Step [1530/1669], Train Loss: 1.1477\nEpoch [1/2], Step [1540/1669], Train Loss: 1.0400\nEpoch [1/2], Step [1550/1669], Train Loss: 1.0431\nEpoch [1/2], Step [1560/1669], Train Loss: 1.1913\nEpoch [1/2], Step [1570/1669], Train Loss: 1.2060\nEpoch [1/2], Step [1580/1669], Train Loss: 1.2052\nEpoch [1/2], Step [1590/1669], Train Loss: 1.2181\nEpoch [1/2], Step [1600/1669], Train Loss: 1.1722\nEpoch [1/2], Step [1610/1669], Train Loss: 1.2771\nEpoch [1/2], Step [1620/1669], Train Loss: 1.2884\nEpoch [1/2], Step [1630/1669], Train Loss: 1.3040\nEpoch [1/2], Step [1640/1669], Train Loss: 1.2267\nEpoch [1/2], Step [1650/1669], Train Loss: 1.3521\nEpoch [1/2], Step [1660/1669], Train Loss: 1.0860\nEpoch 1/2, Loss: 177.7036, Accuracy: 0.5817\nEpoch [2/2], Step [10/1669], Train Loss: 1.2372\nEpoch [2/2], Step [20/1669], Train Loss: 1.2382\nEpoch [2/2], Step [30/1669], Train Loss: 1.1953\nEpoch [2/2], Step [40/1669], Train Loss: 1.0812\nEpoch [2/2], Step [50/1669], Train Loss: 1.0684\nEpoch [2/2], Step [60/1669], Train Loss: 1.2165\nEpoch [2/2], Step [70/1669], Train Loss: 1.2982\nEpoch [2/2], Step [80/1669], Train Loss: 1.1847\nEpoch [2/2], Step [90/1669], Train Loss: 1.2559\nEpoch [2/2], Step [100/1669], Train Loss: 1.3058\nEpoch [2/2], Step [110/1669], Train Loss: 1.0450\nEpoch [2/2], Step [120/1669], Train Loss: 1.1706\nEpoch [2/2], Step [130/1669], Train Loss: 1.0844\nEpoch [2/2], Step [140/1669], Train Loss: 0.9271\nEpoch [2/2], Step [150/1669], Train Loss: 1.1499\nEpoch [2/2], Step [160/1669], Train Loss: 1.5898\nEpoch [2/2], Step [170/1669], Train Loss: 0.8710\nEpoch [2/2], Step [180/1669], Train Loss: 1.2032\nEpoch [2/2], Step [190/1669], Train Loss: 1.3794\nEpoch [2/2], Step [200/1669], Train Loss: 1.0168\nEpoch [2/2], Step [210/1669], Train Loss: 1.0242\nEpoch [2/2], Step [220/1669], Train Loss: 1.1243\nEpoch [2/2], Step [230/1669], Train Loss: 1.2287\nEpoch [2/2], Step [240/1669], Train Loss: 1.1713\nEpoch [2/2], Step [250/1669], Train Loss: 1.0414\nEpoch [2/2], Step [260/1669], Train Loss: 1.2348\nEpoch [2/2], Step [270/1669], Train Loss: 1.0343\nEpoch [2/2], Step [280/1669], Train Loss: 0.9514\nEpoch [2/2], Step [290/1669], Train Loss: 1.2796\nEpoch [2/2], Step [300/1669], Train Loss: 1.1198\nEpoch [2/2], Step [310/1669], Train Loss: 1.2059\nEpoch [2/2], Step [320/1669], Train Loss: 1.1902\nEpoch [2/2], Step [330/1669], Train Loss: 1.4448\nEpoch [2/2], Step [340/1669], Train Loss: 1.1075\nEpoch [2/2], Step [350/1669], Train Loss: 1.1421\nEpoch [2/2], Step [360/1669], Train Loss: 1.1871\nEpoch [2/2], Step [370/1669], Train Loss: 1.2298\nEpoch [2/2], Step [380/1669], Train Loss: 1.0251\nEpoch [2/2], Step [390/1669], Train Loss: 1.0346\nEpoch [2/2], Step [400/1669], Train Loss: 1.0568\nEpoch [2/2], Step [410/1669], Train Loss: 0.9689\nEpoch [2/2], Step [420/1669], Train Loss: 1.2184\nEpoch [2/2], Step [430/1669], Train Loss: 0.9646\nEpoch [2/2], Step [440/1669], Train Loss: 1.0185\nEpoch [2/2], Step [450/1669], Train Loss: 1.1321\nEpoch [2/2], Step [460/1669], Train Loss: 0.8500\nEpoch [2/2], Step [470/1669], Train Loss: 1.1358\nEpoch [2/2], Step [480/1669], Train Loss: 1.3916\nEpoch [2/2], Step [490/1669], Train Loss: 1.1721\nEpoch [2/2], Step [500/1669], Train Loss: 1.2590\nEpoch [2/2], Step [510/1669], Train Loss: 1.0212\nEpoch [2/2], Step [520/1669], Train Loss: 1.0262\nEpoch [2/2], Step [530/1669], Train Loss: 1.1406\nEpoch [2/2], Step [540/1669], Train Loss: 1.4568\nEpoch [2/2], Step [550/1669], Train Loss: 1.4181\nEpoch [2/2], Step [560/1669], Train Loss: 1.0888\nEpoch [2/2], Step [570/1669], Train Loss: 1.0373\nEpoch [2/2], Step [580/1669], Train Loss: 1.1970\nEpoch [2/2], Step [590/1669], Train Loss: 1.1107\nEpoch [2/2], Step [600/1669], Train Loss: 1.0882\nEpoch [2/2], Step [610/1669], Train Loss: 1.2091\nEpoch [2/2], Step [620/1669], Train Loss: 1.2472\nEpoch [2/2], Step [630/1669], Train Loss: 1.1847\nEpoch [2/2], Step [640/1669], Train Loss: 1.0253\nEpoch [2/2], Step [650/1669], Train Loss: 1.1207\nEpoch [2/2], Step [660/1669], Train Loss: 1.4258\nEpoch [2/2], Step [670/1669], Train Loss: 1.1810\nEpoch [2/2], Step [680/1669], Train Loss: 1.0736\nEpoch [2/2], Step [690/1669], Train Loss: 0.9530\nEpoch [2/2], Step [700/1669], Train Loss: 1.0789\nEpoch [2/2], Step [710/1669], Train Loss: 1.0167\nEpoch [2/2], Step [720/1669], Train Loss: 1.2081\nEpoch [2/2], Step [730/1669], Train Loss: 1.0931\nEpoch [2/2], Step [740/1669], Train Loss: 1.1668\nEpoch [2/2], Step [750/1669], Train Loss: 0.9537\nEpoch [2/2], Step [760/1669], Train Loss: 1.3140\nEpoch [2/2], Step [770/1669], Train Loss: 1.1071\nEpoch [2/2], Step [780/1669], Train Loss: 1.2593\nEpoch [2/2], Step [790/1669], Train Loss: 1.0534\nEpoch [2/2], Step [800/1669], Train Loss: 1.2604\nEpoch [2/2], Step [810/1669], Train Loss: 1.0790\nEpoch [2/2], Step [820/1669], Train Loss: 1.2305\nEpoch [2/2], Step [830/1669], Train Loss: 1.0293\nEpoch [2/2], Step [840/1669], Train Loss: 1.0015\nEpoch [2/2], Step [850/1669], Train Loss: 0.9813\nEpoch [2/2], Step [860/1669], Train Loss: 1.2127\nEpoch [2/2], Step [870/1669], Train Loss: 1.1734\nEpoch [2/2], Step [880/1669], Train Loss: 1.1209\nEpoch [2/2], Step [890/1669], Train Loss: 1.1145\nEpoch [2/2], Step [900/1669], Train Loss: 0.9572\nEpoch [2/2], Step [910/1669], Train Loss: 1.2248\nEpoch [2/2], Step [920/1669], Train Loss: 1.1417\nEpoch [2/2], Step [930/1669], Train Loss: 1.0256\nEpoch [2/2], Step [940/1669], Train Loss: 1.2353\nEpoch [2/2], Step [950/1669], Train Loss: 1.1753\nEpoch [2/2], Step [960/1669], Train Loss: 0.9763\nEpoch [2/2], Step [970/1669], Train Loss: 1.1812\nEpoch [2/2], Step [980/1669], Train Loss: 1.3005\nEpoch [2/2], Step [990/1669], Train Loss: 1.1690\nEpoch [2/2], Step [1000/1669], Train Loss: 1.1452\nEpoch [2/2], Step [1010/1669], Train Loss: 1.0376\nEpoch [2/2], Step [1020/1669], Train Loss: 1.0230\nEpoch [2/2], Step [1030/1669], Train Loss: 1.2792\nEpoch [2/2], Step [1040/1669], Train Loss: 1.3295\nEpoch [2/2], Step [1050/1669], Train Loss: 1.2430\nEpoch [2/2], Step [1060/1669], Train Loss: 1.2225\nEpoch [2/2], Step [1070/1669], Train Loss: 1.0141\nEpoch [2/2], Step [1080/1669], Train Loss: 1.1619\nEpoch [2/2], Step [1090/1669], Train Loss: 1.1927\nEpoch [2/2], Step [1100/1669], Train Loss: 1.2742\nEpoch [2/2], Step [1110/1669], Train Loss: 1.1407\nEpoch [2/2], Step [1120/1669], Train Loss: 0.9609\nEpoch [2/2], Step [1130/1669], Train Loss: 1.3274\nEpoch [2/2], Step [1140/1669], Train Loss: 1.2030\nEpoch [2/2], Step [1150/1669], Train Loss: 1.3007\nEpoch [2/2], Step [1160/1669], Train Loss: 1.0293\nEpoch [2/2], Step [1170/1669], Train Loss: 1.0939\nEpoch [2/2], Step [1180/1669], Train Loss: 1.0056\nEpoch [2/2], Step [1190/1669], Train Loss: 1.3296\nEpoch [2/2], Step [1200/1669], Train Loss: 1.0664\nEpoch [2/2], Step [1210/1669], Train Loss: 1.2090\nEpoch [2/2], Step [1220/1669], Train Loss: 0.9421\nEpoch [2/2], Step [1230/1669], Train Loss: 0.9636\nEpoch [2/2], Step [1240/1669], Train Loss: 1.0886\nEpoch [2/2], Step [1250/1669], Train Loss: 0.9921\nEpoch [2/2], Step [1260/1669], Train Loss: 1.0033\nEpoch [2/2], Step [1270/1669], Train Loss: 1.1424\nEpoch [2/2], Step [1280/1669], Train Loss: 1.1549\nEpoch [2/2], Step [1290/1669], Train Loss: 1.0971\nEpoch [2/2], Step [1300/1669], Train Loss: 1.2034\nEpoch [2/2], Step [1310/1669], Train Loss: 1.2548\nEpoch [2/2], Step [1320/1669], Train Loss: 1.0903\nEpoch [2/2], Step [1330/1669], Train Loss: 0.9873\nEpoch [2/2], Step [1340/1669], Train Loss: 0.9833\nEpoch [2/2], Step [1350/1669], Train Loss: 1.0878\nEpoch [2/2], Step [1360/1669], Train Loss: 1.1048\nEpoch [2/2], Step [1370/1669], Train Loss: 0.9954\nEpoch [2/2], Step [1380/1669], Train Loss: 1.0384\nEpoch [2/2], Step [1390/1669], Train Loss: 1.2338\nEpoch [2/2], Step [1400/1669], Train Loss: 1.0708\nEpoch [2/2], Step [1410/1669], Train Loss: 1.2742\nEpoch [2/2], Step [1420/1669], Train Loss: 1.0391\nEpoch [2/2], Step [1430/1669], Train Loss: 1.1555\nEpoch [2/2], Step [1440/1669], Train Loss: 1.2075\nEpoch [2/2], Step [1450/1669], Train Loss: 1.1836\nEpoch [2/2], Step [1460/1669], Train Loss: 0.9604\nEpoch [2/2], Step [1470/1669], Train Loss: 1.0862\nEpoch [2/2], Step [1480/1669], Train Loss: 1.2079\nEpoch [2/2], Step [1490/1669], Train Loss: 1.0801\nEpoch [2/2], Step [1500/1669], Train Loss: 1.1848\nEpoch [2/2], Step [1510/1669], Train Loss: 1.1860\nEpoch [2/2], Step [1520/1669], Train Loss: 1.2319\nEpoch [2/2], Step [1530/1669], Train Loss: 0.8762\nEpoch [2/2], Step [1540/1669], Train Loss: 1.1600\nEpoch [2/2], Step [1550/1669], Train Loss: 1.2026\nEpoch [2/2], Step [1560/1669], Train Loss: 1.2603\nEpoch [2/2], Step [1570/1669], Train Loss: 1.1315\nEpoch [2/2], Step [1580/1669], Train Loss: 1.0833\nEpoch [2/2], Step [1590/1669], Train Loss: 1.1369\nEpoch [2/2], Step [1600/1669], Train Loss: 1.1927\nEpoch [2/2], Step [1610/1669], Train Loss: 1.2637\nEpoch [2/2], Step [1620/1669], Train Loss: 1.2224\nEpoch [2/2], Step [1630/1669], Train Loss: 1.2291\nEpoch [2/2], Step [1640/1669], Train Loss: 1.0991\nEpoch [2/2], Step [1650/1669], Train Loss: 0.9906\nEpoch [2/2], Step [1660/1669], Train Loss: 1.2502\nEpoch 2/2, Loss: 148.4986, Accuracy: 0.6368\n","output_type":"stream"}]}]}