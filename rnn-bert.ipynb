{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom transformers import AutoTokenizer, AutoModel","metadata":{"execution":{"iopub.status.busy":"2023-03-12T03:28:55.769972Z","iopub.execute_input":"2023-03-12T03:28:55.770778Z","iopub.status.idle":"2023-03-12T03:29:06.835841Z","shell.execute_reply.started":"2023-03-12T03:28:55.770738Z","shell.execute_reply":"2023-03-12T03:29:06.834728Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModel, BertTokenizer, BertModel\nimport torch\nfrom torch.utils.data import DataLoader, Dataset\nfrom datasets import load_dataset","metadata":{"execution":{"iopub.status.busy":"2023-03-12T03:29:06.838220Z","iopub.execute_input":"2023-03-12T03:29:06.838967Z","iopub.status.idle":"2023-03-12T03:29:07.735509Z","shell.execute_reply.started":"2023-03-12T03:29:06.838928Z","shell.execute_reply":"2023-03-12T03:29:07.734404Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# Load the SWDA dataset\nswda = load_dataset(\"swda\")\n\nswda","metadata":{"execution":{"iopub.status.busy":"2023-03-12T03:29:07.737270Z","iopub.execute_input":"2023-03-12T03:29:07.738210Z","iopub.status.idle":"2023-03-12T03:30:32.278108Z","shell.execute_reply.started":"2023-03-12T03:29:07.738174Z","shell.execute_reply":"2023-03-12T03:30:32.275947Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/7.11k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"62290efcf6a34674b07b8c65de3f8d3e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading metadata:   0%|          | 0.00/2.54k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aa43f54acd4047a29719113cd375acfa"}},"metadata":{}},{"name":"stdout","text":"Downloading and preparing dataset swda/default (download: 13.79 MiB, generated: 158.13 MiB, post-processed: Unknown size, total: 171.91 MiB) to /root/.cache/huggingface/datasets/swda/default/0.0.0/b53d17ec4c6e31d0921591dd2d8e86d15850822209a980fcddb2983fc948e499...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/14.4M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5c93a9ccd40947109cd66155fce02635"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data files:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"812ecce37f414d248e26f32fd0914647"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/2.24k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3be62188a99648a2ae572a478fcbaade"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/579 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"88ad3f95ab1d4c599b9ec5edb8edd8ff"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/75.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1daf7d44aebf45768e4572d67a6a4232"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting data files:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e08c712153524ae4bb3c466d2bf5b82e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/213543 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/56729 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/4514 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Dataset swda downloaded and prepared to /root/.cache/huggingface/datasets/swda/default/0.0.0/b53d17ec4c6e31d0921591dd2d8e86d15850822209a980fcddb2983fc948e499. Subsequent calls will reuse this data.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7684779b68bf4cb094e339c9d65a2296"}},"metadata":{}},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['swda_filename', 'ptb_basename', 'conversation_no', 'transcript_index', 'act_tag', 'damsl_act_tag', 'caller', 'utterance_index', 'subutterance_index', 'text', 'pos', 'trees', 'ptb_treenumbers', 'talk_day', 'length', 'topic_description', 'prompt', 'from_caller', 'from_caller_sex', 'from_caller_education', 'from_caller_birth_year', 'from_caller_dialect_area', 'to_caller', 'to_caller_sex', 'to_caller_education', 'to_caller_birth_year', 'to_caller_dialect_area'],\n        num_rows: 213543\n    })\n    validation: Dataset({\n        features: ['swda_filename', 'ptb_basename', 'conversation_no', 'transcript_index', 'act_tag', 'damsl_act_tag', 'caller', 'utterance_index', 'subutterance_index', 'text', 'pos', 'trees', 'ptb_treenumbers', 'talk_day', 'length', 'topic_description', 'prompt', 'from_caller', 'from_caller_sex', 'from_caller_education', 'from_caller_birth_year', 'from_caller_dialect_area', 'to_caller', 'to_caller_sex', 'to_caller_education', 'to_caller_birth_year', 'to_caller_dialect_area'],\n        num_rows: 56729\n    })\n    test: Dataset({\n        features: ['swda_filename', 'ptb_basename', 'conversation_no', 'transcript_index', 'act_tag', 'damsl_act_tag', 'caller', 'utterance_index', 'subutterance_index', 'text', 'pos', 'trees', 'ptb_treenumbers', 'talk_day', 'length', 'topic_description', 'prompt', 'from_caller', 'from_caller_sex', 'from_caller_education', 'from_caller_birth_year', 'from_caller_dialect_area', 'to_caller', 'to_caller_sex', 'to_caller_education', 'to_caller_birth_year', 'to_caller_dialect_area'],\n        num_rows: 4514\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"# Define the data preprocessing function\ndef preprocess_data(utterance):\n    text = utterance['text']\n    label = utterance['damsl_act_tag']\n    return {'text': text, 'label': label}\n# Preprocess the data\nswda = swda.map(preprocess_data)","metadata":{"execution":{"iopub.status.busy":"2023-03-12T03:30:32.280775Z","iopub.execute_input":"2023-03-12T03:30:32.281757Z","iopub.status.idle":"2023-03-12T03:31:37.307795Z","shell.execute_reply.started":"2023-03-12T03:30:32.281720Z","shell.execute_reply":"2023-03-12T03:31:37.306877Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/213543 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7ab695126d2042c6b10d651fe7074d2c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/56729 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d2389a24c40c4964a97b31e0894d0b97"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/4514 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"36e65be74c1444eda24145be8d708ed4"}},"metadata":{}}]},{"cell_type":"code","source":"swda = swda.remove_columns(['swda_filename', 'ptb_basename', 'conversation_no', 'transcript_index', 'act_tag', 'damsl_act_tag', 'caller', 'utterance_index', 'subutterance_index', 'pos', 'trees', 'ptb_treenumbers', 'talk_day', 'length', 'topic_description', 'prompt', 'from_caller', 'from_caller_sex', 'from_caller_education', 'from_caller_birth_year', 'from_caller_dialect_area', 'to_caller', 'to_caller_sex', 'to_caller_education', 'to_caller_birth_year', 'to_caller_dialect_area'])","metadata":{"execution":{"iopub.status.busy":"2023-03-12T03:31:37.309386Z","iopub.execute_input":"2023-03-12T03:31:37.309779Z","iopub.status.idle":"2023-03-12T03:31:37.332260Z","shell.execute_reply.started":"2023-03-12T03:31:37.309740Z","shell.execute_reply":"2023-03-12T03:31:37.330960Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"swda","metadata":{"execution":{"iopub.status.busy":"2023-03-12T03:31:37.335390Z","iopub.execute_input":"2023-03-12T03:31:37.335980Z","iopub.status.idle":"2023-03-12T03:31:37.402670Z","shell.execute_reply.started":"2023-03-12T03:31:37.335936Z","shell.execute_reply":"2023-03-12T03:31:37.401602Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['text', 'label'],\n        num_rows: 213543\n    })\n    validation: Dataset({\n        features: ['text', 'label'],\n        num_rows: 56729\n    })\n    test: Dataset({\n        features: ['text', 'label'],\n        num_rows: 4514\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"import pandas as pd\nsorted(pd.DataFrame(swda[\"train\"])[\"label\"].unique())","metadata":{"execution":{"iopub.status.busy":"2023-03-12T03:31:37.404378Z","iopub.execute_input":"2023-03-12T03:31:37.405433Z","iopub.status.idle":"2023-03-12T03:31:49.046903Z","shell.execute_reply.started":"2023-03-12T03:31:37.405395Z","shell.execute_reply":"2023-03-12T03:31:49.045853Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"[0,\n 1,\n 2,\n 3,\n 4,\n 5,\n 6,\n 7,\n 8,\n 9,\n 10,\n 11,\n 12,\n 13,\n 14,\n 15,\n 16,\n 17,\n 18,\n 19,\n 20,\n 21,\n 22,\n 23,\n 24,\n 25,\n 26,\n 27,\n 28,\n 29,\n 30,\n 31,\n 32,\n 33,\n 34,\n 35,\n 36,\n 37,\n 38,\n 39,\n 40,\n 41,\n 42]"},"metadata":{}}]},{"cell_type":"code","source":"# Tokenize the input and context\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nswda = swda.map(lambda x: tokenizer(x['text'], padding='max_length', truncation=True), batched=True)","metadata":{"execution":{"iopub.status.busy":"2023-03-12T03:31:49.048509Z","iopub.execute_input":"2023-03-12T03:31:49.049173Z","iopub.status.idle":"2023-03-12T03:34:11.288342Z","shell.execute_reply.started":"2023-03-12T03:31:49.049132Z","shell.execute_reply":"2023-03-12T03:34:11.287159Z"},"trusted":true},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"65bc7c37ceff41d9b904e5ff414c1adb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8661c4fba2014ef6b490ce2e8c730734"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a269a82d5115436f8b004df9470623b2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/214 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"de664b0c95ac4ca2a80ce9c8412c4eea"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/57 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5ef753b2ff124ef7a37bf4e9405426e8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/5 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"975f06bc22ab4f6ab598b41b31b37e49"}},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModel\nimport torch\nfrom torch.utils.data import DataLoader, Dataset\nfrom datasets import load_dataset\n\n\nclass SWDADataset(Dataset):\n    def __init__(self, swda, tokenizer, max_seq_length):\n        self.swda = swda\n        self.tokenizer = tokenizer\n        self.max_seq_length = max_seq_length\n\n    def __len__(self):\n        return len(self.swda)\n\n    def __getitem__(self, index):\n        # Get input text\n        input_text = self.tokenizer.encode_plus(\n            self.swda[index]['text'],\n            add_special_tokens=True,\n            max_length=self.max_seq_length,\n            padding='max_length',\n            truncation=True,\n            return_tensors='pt'\n        )\n\n        # Get context\n        context = self.swda[index-1] if index > 0 else self.swda[index]\n        context = self.tokenizer.encode_plus(\n            context['text'],\n            add_special_tokens=True,\n            max_length=self.max_seq_length,\n            padding='max_length',\n            truncation=True,\n            return_tensors='pt'\n        )\n\n        # Get label\n        label = self.swda[index]['label']\n        \n        return input_text, context, label\n\n\n# Create the train loader\ntrain_dataset = SWDADataset(swda['train'], tokenizer, max_seq_length=128)\nval_dataset = SWDADataset(swda['validation'], tokenizer, max_seq_length=128)\ntest_dataset = SWDADataset(swda['test'], tokenizer, max_seq_length=128)\n\ntrain_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\nval_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=64, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2023-03-12T03:34:11.289738Z","iopub.execute_input":"2023-03-12T03:34:11.290193Z","iopub.status.idle":"2023-03-12T03:34:11.301397Z","shell.execute_reply.started":"2023-03-12T03:34:11.290155Z","shell.execute_reply":"2023-03-12T03:34:11.300307Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"train_dataset[0]","metadata":{"execution":{"iopub.status.busy":"2023-03-12T03:34:11.305612Z","iopub.execute_input":"2023-03-12T03:34:11.306305Z","iopub.status.idle":"2023-03-12T03:34:11.370988Z","shell.execute_reply.started":"2023-03-12T03:34:11.306253Z","shell.execute_reply":"2023-03-12T03:34:11.370053Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"({'input_ids': tensor([[ 101, 3100, 1012, 1013,  102,    0,    0,    0,    0,    0,    0,    0,\n             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n             0,    0,    0,    0,    0,    0,    0,    0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0]])},\n {'input_ids': tensor([[ 101, 3100, 1012, 1013,  102,    0,    0,    0,    0,    0,    0,    0,\n             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n             0,    0,    0,    0,    0,    0,    0,    0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0]])},\n 26)"},"metadata":{}}]},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"execution":{"iopub.status.busy":"2023-03-12T03:34:11.372413Z","iopub.execute_input":"2023-03-12T03:34:11.372749Z","iopub.status.idle":"2023-03-12T03:34:11.439489Z","shell.execute_reply.started":"2023-03-12T03:34:11.372712Z","shell.execute_reply":"2023-03-12T03:34:11.437914Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# Define the model architecture\n# Define model architecture\nclass RNN(nn.Module):\n    def __init__(self, hidden_size, num_classes):\n        super(RNN, self).__init__()\n        self.bert = BertModel.from_pretrained('bert-base-uncased')\n        for param in self.bert.parameters():\n            param.requires_grad = False\n        self.rnn = nn.GRU(input_size=768, hidden_size=hidden_size, batch_first=True)\n        self.fc1 = nn.Linear(hidden_size, hidden_size)\n        self.fc2 = nn.Linear(hidden_size * 2, num_classes)\n\n    def forward(self, input_text, context):\n        input_ids = input_text['input_ids'].to(device)\n        attention_mask = input_text['attention_mask'].to(device)\n        context_ids = context['input_ids'].to(device)\n        context_mask = context['attention_mask'].to(device)\n        \n        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask, return_dict=True)\n        pooled_output = outputs.pooler_output\n\n        context_outputs = self.bert(input_ids=context_ids, attention_mask=context_mask, return_dict=True)\n        pooled_context_output = context_outputs.pooler_output\n        \n        \n        output, _ = self.rnn(pooled_output.unsqueeze(1))\n        context_output, _ = self.rnn(pooled_context_output.unsqueeze(1))\n        output = self.fc1(output)\n        output = torch.cat((output.squeeze(1), context_output.squeeze(1)), dim=1)\n        output = self.fc2(output)\n        return output\n\n# Instantiate the model\n# Instantiate model and move it to device\nmodel = RNN(hidden_size=256, num_classes=43)\nmodel = model.to(device)","metadata":{"execution":{"iopub.status.busy":"2023-03-12T03:34:11.442332Z","iopub.execute_input":"2023-03-12T03:34:11.443093Z","iopub.status.idle":"2023-03-12T03:34:20.313582Z","shell.execute_reply.started":"2023-03-12T03:34:11.443051Z","shell.execute_reply":"2023-03-12T03:34:20.312464Z"},"trusted":true},"execution_count":12,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)\"pytorch_model.bin\";:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"868c164f06c847589266455e54f380e0"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"}]},{"cell_type":"code","source":"# Define loss function and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)","metadata":{"execution":{"iopub.status.busy":"2023-03-12T03:34:20.314927Z","iopub.execute_input":"2023-03-12T03:34:20.315614Z","iopub.status.idle":"2023-03-12T03:34:20.322407Z","shell.execute_reply.started":"2023-03-12T03:34:20.315570Z","shell.execute_reply":"2023-03-12T03:34:20.321347Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"import os\n#os.environ[\"CUDA_LAUNCH_BLOCKING\"]= \"true\"","metadata":{"execution":{"iopub.status.busy":"2023-03-12T03:34:20.324134Z","iopub.execute_input":"2023-03-12T03:34:20.324842Z","iopub.status.idle":"2023-03-12T03:34:20.505280Z","shell.execute_reply.started":"2023-03-12T03:34:20.324804Z","shell.execute_reply":"2023-03-12T03:34:20.503663Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"num_epochs = 5\n\nbest_val_loss = float('inf')\nfor epoch in range(num_epochs):\n    epoch_loss = 0\n    epoch_acc = 0\n\n    model.train()\n\n    for i, (input_text, context, labels) in enumerate(train_loader):\n        optimizer.zero_grad()\n\n        input_text = {\n            'input_ids': input_text['input_ids'].squeeze(1),\n            'attention_mask': input_text['attention_mask'].squeeze(1),\n            'token_type_ids': input_text['token_type_ids'].squeeze(1)\n        }\n        context = {\n            'input_ids': context['input_ids'].squeeze(1),\n            'attention_mask': context['attention_mask'].squeeze(1),\n            'token_type_ids': context['token_type_ids'].squeeze(1)\n        }\n        labels = labels.to(device)\n        output = model(input_text, context)\n        loss = criterion(output, labels)\n\n        loss.backward()\n        optimizer.step()\n\n        epoch_loss += loss.item() * input_text[\"input_ids\"].size(0)\n\n        # Compute accuracy for this batch\n        predicted_labels = output.argmax(dim=1)\n        num_correct = (predicted_labels == labels).sum().item()\n        acc = num_correct / len(labels)\n        epoch_acc += acc\n        if (i+1) % 100 == 0:\n            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Train Loss: {loss.item():.4f}')\n\n    # Compute average loss and accuracy for epoch\n    epoch_loss /= len(train_loader)\n    epoch_acc /= len(train_loader)\n\n    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.4f}')\n    \n    # Validation loop\n    model.eval()\n    val_loss = 0\n    val_acc = 0\n    with torch.no_grad():\n        for input_text, context, labels in val_loader:\n            input_text = {\n                'input_ids': input_text['input_ids'].squeeze(1),\n                'attention_mask': input_text['attention_mask'].squeeze(1),\n                'token_type_ids': input_text['token_type_ids'].squeeze(1)\n            }\n            context = {\n                'input_ids': context['input_ids'].squeeze(1),\n                'attention_mask': context['attention_mask'].squeeze(1),\n                'token_type_ids': context['token_type_ids'].squeeze(1)\n            }\n            labels = labels.to(device)\n            output = model(input_text, context)\n            loss = criterion(output, labels)\n\n            val_loss += loss.item() * input_text[\"input_ids\"].size(0)\n\n            # Compute accuracy for this batch\n            predicted_labels = output.argmax(dim=1)\n            num_correct = (predicted_labels == labels).sum().item()\n            acc = num_correct / len(labels)\n            val_acc += acc\n\n    # Compute average validation loss and accuracy\n    val_loss /= len(val_loader)\n    val_acc /= len(val_loader)\n    \n    print(f'Epoch {epoch+1}/{num_epochs}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_acc:.4f}')\n\n    # Save best model\n    if val_loss < best_val_loss:\n        best_val_loss = val_loss\n        torch.save(model.state_dict(), 'best_model.pt')\n","metadata":{"execution":{"iopub.status.busy":"2023-03-12T03:34:20.509372Z","iopub.execute_input":"2023-03-12T03:34:20.510918Z","iopub.status.idle":"2023-03-12T09:26:07.428772Z","shell.execute_reply.started":"2023-03-12T03:34:20.510878Z","shell.execute_reply":"2023-03-12T09:26:07.426788Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"Epoch [1/5], Step [100/1669], Train Loss: 1.8914\nEpoch [1/5], Step [200/1669], Train Loss: 1.8300\nEpoch [1/5], Step [300/1669], Train Loss: 1.5354\nEpoch [1/5], Step [400/1669], Train Loss: 1.4443\nEpoch [1/5], Step [500/1669], Train Loss: 1.5466\nEpoch [1/5], Step [600/1669], Train Loss: 1.3846\nEpoch [1/5], Step [700/1669], Train Loss: 1.2945\nEpoch [1/5], Step [800/1669], Train Loss: 1.3506\nEpoch [1/5], Step [900/1669], Train Loss: 1.5413\nEpoch [1/5], Step [1000/1669], Train Loss: 1.2017\nEpoch [1/5], Step [1100/1669], Train Loss: 1.1159\nEpoch [1/5], Step [1200/1669], Train Loss: 1.3620\nEpoch [1/5], Step [1300/1669], Train Loss: 1.1618\nEpoch [1/5], Step [1400/1669], Train Loss: 1.1463\nEpoch [1/5], Step [1500/1669], Train Loss: 1.2350\nEpoch [1/5], Step [1600/1669], Train Loss: 1.1519\nEpoch 1/5, Loss: 176.2643, Accuracy: 0.5839\nEpoch 1/5, Val Loss: 75.4238, Val Accuracy: 0.6367\nEpoch [2/5], Step [100/1669], Train Loss: 1.0988\nEpoch [2/5], Step [200/1669], Train Loss: 1.0906\nEpoch [2/5], Step [300/1669], Train Loss: 1.1537\nEpoch [2/5], Step [400/1669], Train Loss: 1.1505\nEpoch [2/5], Step [500/1669], Train Loss: 1.2561\nEpoch [2/5], Step [600/1669], Train Loss: 1.1752\nEpoch [2/5], Step [700/1669], Train Loss: 1.4586\nEpoch [2/5], Step [800/1669], Train Loss: 1.2949\nEpoch [2/5], Step [900/1669], Train Loss: 0.8635\nEpoch [2/5], Step [1000/1669], Train Loss: 1.3318\nEpoch [2/5], Step [1100/1669], Train Loss: 1.3755\nEpoch [2/5], Step [1200/1669], Train Loss: 0.9439\nEpoch [2/5], Step [1300/1669], Train Loss: 1.0296\nEpoch [2/5], Step [1400/1669], Train Loss: 1.2912\nEpoch [2/5], Step [1500/1669], Train Loss: 1.4019\nEpoch [2/5], Step [1600/1669], Train Loss: 1.0483\nEpoch 2/5, Loss: 148.4307, Accuracy: 0.6372\nEpoch 2/5, Val Loss: 67.7729, Val Accuracy: 0.6623\nEpoch [3/5], Step [100/1669], Train Loss: 1.2860\nEpoch [3/5], Step [200/1669], Train Loss: 1.1086\nEpoch [3/5], Step [300/1669], Train Loss: 1.0779\nEpoch [3/5], Step [400/1669], Train Loss: 1.3183\nEpoch [3/5], Step [500/1669], Train Loss: 1.1579\nEpoch [3/5], Step [600/1669], Train Loss: 1.1168\nEpoch [3/5], Step [700/1669], Train Loss: 1.1667\nEpoch [3/5], Step [800/1669], Train Loss: 0.9047\nEpoch [3/5], Step [900/1669], Train Loss: 1.1028\nEpoch [3/5], Step [1000/1669], Train Loss: 1.0579\nEpoch [3/5], Step [1100/1669], Train Loss: 1.3285\nEpoch [3/5], Step [1200/1669], Train Loss: 0.9506\nEpoch [3/5], Step [1300/1669], Train Loss: 0.9604\nEpoch [3/5], Step [1400/1669], Train Loss: 1.0730\nEpoch [3/5], Step [1500/1669], Train Loss: 1.1345\nEpoch [3/5], Step [1600/1669], Train Loss: 1.4136\nEpoch 3/5, Loss: 142.5143, Accuracy: 0.6498\nEpoch 3/5, Val Loss: 63.9673, Val Accuracy: 0.6808\nEpoch [4/5], Step [100/1669], Train Loss: 0.9940\nEpoch [4/5], Step [200/1669], Train Loss: 1.0327\nEpoch [4/5], Step [300/1669], Train Loss: 1.0574\nEpoch [4/5], Step [400/1669], Train Loss: 1.1853\nEpoch [4/5], Step [500/1669], Train Loss: 1.0322\nEpoch [4/5], Step [600/1669], Train Loss: 0.9687\nEpoch [4/5], Step [700/1669], Train Loss: 1.1380\nEpoch [4/5], Step [800/1669], Train Loss: 1.0475\nEpoch [4/5], Step [900/1669], Train Loss: 0.9643\nEpoch [4/5], Step [1000/1669], Train Loss: 1.0240\nEpoch [4/5], Step [1100/1669], Train Loss: 1.3711\nEpoch [4/5], Step [1200/1669], Train Loss: 1.2847\nEpoch [4/5], Step [1300/1669], Train Loss: 1.1444\nEpoch [4/5], Step [1400/1669], Train Loss: 1.1113\nEpoch [4/5], Step [1500/1669], Train Loss: 1.2795\nEpoch [4/5], Step [1600/1669], Train Loss: 1.1164\nEpoch 4/5, Loss: 138.5493, Accuracy: 0.6582\nEpoch 4/5, Val Loss: 60.4562, Val Accuracy: 0.7026\nEpoch [5/5], Step [100/1669], Train Loss: 0.8941\nEpoch [5/5], Step [200/1669], Train Loss: 0.9941\nEpoch [5/5], Step [300/1669], Train Loss: 1.0663\nEpoch [5/5], Step [400/1669], Train Loss: 1.0599\nEpoch [5/5], Step [500/1669], Train Loss: 1.1698\nEpoch [5/5], Step [600/1669], Train Loss: 0.9972\nEpoch [5/5], Step [700/1669], Train Loss: 1.0397\nEpoch [5/5], Step [800/1669], Train Loss: 1.1054\nEpoch [5/5], Step [900/1669], Train Loss: 0.9657\nEpoch [5/5], Step [1000/1669], Train Loss: 1.0238\nEpoch [5/5], Step [1100/1669], Train Loss: 0.9816\nEpoch [5/5], Step [1200/1669], Train Loss: 1.1049\nEpoch [5/5], Step [1300/1669], Train Loss: 1.0716\nEpoch [5/5], Step [1400/1669], Train Loss: 1.0637\nEpoch [5/5], Step [1500/1669], Train Loss: 1.0464\nEpoch [5/5], Step [1600/1669], Train Loss: 1.2046\nEpoch 5/5, Loss: 135.9098, Accuracy: 0.6628\nEpoch 5/5, Val Loss: 62.0134, Val Accuracy: 0.6960\n","output_type":"stream"}]},{"cell_type":"code","source":"model.eval()\ntest_loss = 0\ntest_acc = 0\nwith torch.no_grad():\n    for input_text, context, labels in test_loader:\n        input_text = {\n            'input_ids': input_text['input_ids'].squeeze(1),\n            'attention_mask': input_text['attention_mask'].squeeze(1),\n            'token_type_ids': input_text['token_type_ids'].squeeze(1)\n        }\n        context = {\n            'input_ids': context['input_ids'].squeeze(1),\n            'attention_mask': context['attention_mask'].squeeze(1),\n            'token_type_ids': context['token_type_ids'].squeeze(1)\n        }\n        labels = labels.to(device)\n        output = model(input_text, context)\n        loss = criterion(output, labels)\n\n        test_loss += loss.item() * input_text[\"input_ids\"].size(0)\n\n        # Compute accuracy for this batch\n        predicted_labels = output.argmax(dim=1)\n        num_correct = (predicted_labels == labels).sum().item()\n        acc = num_correct / len(labels)\n        test_acc += acc\n\n# Compute average test loss and accuracy\ntest_loss /= len(test_loader)\ntest_acc /= len(test_loader)\n\nprint(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.4f}')\n","metadata":{"execution":{"iopub.status.busy":"2023-03-12T09:26:07.430867Z","iopub.execute_input":"2023-03-12T09:26:07.432006Z","iopub.status.idle":"2023-03-12T09:26:52.454259Z","shell.execute_reply.started":"2023-03-12T09:26:07.431963Z","shell.execute_reply":"2023-03-12T09:26:52.453146Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"Test Loss: 66.5564, Test Accuracy: 0.6716\n","output_type":"stream"}]}]}